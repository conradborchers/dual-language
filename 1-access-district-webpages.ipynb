{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycld2 as cld2\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import signal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "GLOBAL_FAILED_URLS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return 0\n",
    "\n",
    "def read_in_last_store():\n",
    "    files = glob.glob('pages_*')\n",
    "    if len(files) == 0:\n",
    "        return pd.DataFrame({\n",
    "            'url': [], 'scrape_data': [], 'nces_id': []\n",
    "            })\n",
    "    sorted_files = sorted(files, key=extract_number)\n",
    "    latest_file = sorted_files[-1]\n",
    "    return pd.read_csv(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0feb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls():\n",
    "    all_districts = pd.read_csv(\"elsi-district-2021-2022.csv\", skiprows = 6)\n",
    "    #all_districts = pd.read_csv(\"elsi-school-2021-2022.csv\", skiprows = 6)\n",
    "    urls = all_districts['Web Site URL [District] 2021-22']\n",
    "    #urls = all_districts['Web Site URL [Public School] 2021-22']\n",
    "    urls = pd.unique(urls)\n",
    "    urls = [u for u in urls if not pd.isna(u) and 'http' in u]\n",
    "    return urls\n",
    "\n",
    "def get_url_map():\n",
    "    urls = get_urls()\n",
    "    #all_districts = pd.read_csv(\"elsi-district-2021-2022.csv\", skiprows = 6)\n",
    "    all_districts = pd.read_csv(\"elsi-school-2021-2022.csv\", skiprows = 6)\n",
    "    all_districts[[\n",
    "        #'Agency ID - NCES Assigned [District] Latest available year',\n",
    "        'School ID - NCES Assigned [Public School] Latest available year',\n",
    "        #'Web Site URL [District] 2021-22'\n",
    "        'Web Site URL [Public School] 2021-22'\n",
    "    ]]\n",
    "    district_dict = {}\n",
    "    for _, row in all_districts.iterrows():\n",
    "        #url = row['Web Site URL [District] 2021-22']\n",
    "        url = row['Web Site URL [Public School] 2021-22']\n",
    "        #district_dict[url] = row['Agency ID - NCES Assigned [District] Latest available year']\n",
    "        district_dict[url] = row['School ID - NCES Assigned [Public School] Latest available year']\n",
    "    return district_dict\n",
    "\n",
    "def send_request(url, retries=1, delay=1):\n",
    "    def timeout_handler(signum, frame):\n",
    "        return None\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    # Set the alarm for 10 seconds\n",
    "    signal.alarm(10)\n",
    "    try:\n",
    "        if url in GLOBAL_FAILED_URLS:\n",
    "            return None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                return soup\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request to {url} failed. Retrying... ({attempt+1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "        print(\"Max retries exceeded. Request failed.\")\n",
    "        GLOBAL_FAILED_URLS.append(url)\n",
    "        return None \n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "def scrape_data_with_subpages(url):\n",
    "    soup_main = send_request(url)\n",
    "    soup_subs = []\n",
    "    if soup_main:\n",
    "        subpage_links = soup_main.find_all('a', href=True)\n",
    "        for link in subpage_links[:10]:\n",
    "            subpage_url = link['href']\n",
    "            if subpage_url.startswith('#'):\n",
    "                continue\n",
    "            if subpage_url.startswith('tel:'):\n",
    "                continue\n",
    "            if subpage_url.startswith('mailto'):\n",
    "                continue\n",
    "            if subpage_url.startswith('/'):\n",
    "                subpage_url = url + subpage_url[1:]  # Construct full URL\n",
    "            soup_subs.append(send_request(subpage_url))\n",
    "    res = {'soup_main': soup_main, 'soup_subs': soup_subs}\n",
    "    return res\n",
    "\n",
    "def scrape_data_without_subpages(url):\n",
    "    soup_main = send_request(url)\n",
    "    res = {'soup_main': soup_main}\n",
    "    return res\n",
    "\n",
    "def save_progress(i='all'):\n",
    "    data = dict()\n",
    "    dmap = get_url_map()\n",
    "    durls, ddat, dnces = [], [], []\n",
    "    dfs = []\n",
    "    for url, dat in tqdm(zip(processed_urls, responses)):\n",
    "        durls.append(url)\n",
    "        ddat.append(dat),\n",
    "        dnces.append(dmap[url])\n",
    "    ddd = pd.DataFrame({\n",
    "        'url': durls,\n",
    "        'scrape_data': [d.get('soup_main') if isinstance(d, dict) else d for d in ddat],\n",
    "        'nces_id': dnces\n",
    "    })\n",
    "    ddd.to_csv(f'pages_{i}.csv', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04432f-d3fc-4047-a938-be5ce6f8738d",
   "metadata": {},
   "source": [
    "## Scraping of subpages (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbfbdb5-7e82-48b8-87f8-4405a644e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'tmp.csv' # Add a sample of webpages (columns: Name {str}) to scrape subpages\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "processed_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dab6ad-c2a7-4fed-a85a-23d01e97e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, u in tqdm(enumerate(df.Name.values, 1)):\n",
    "    try:\n",
    "        responses.append(scrape_data_with_subpages(u))\n",
    "    except:\n",
    "        responses.append('SCRAPING FAILED')\n",
    "    processed_urls.append(u)\n",
    "    if i%1000 == 0:\n",
    "        save_progress(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_TERMS = set([\n",
    "'dual language',\n",
    "'dl',\n",
    "'dual-language',\n",
    "'two-way',\n",
    "'duallanguage',\n",
    "'twoway',\n",
    "'two way',\n",
    "'language immersion'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c127fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag dual language mentions\n",
    "def has_dl_string_list(tokens):\n",
    "    try:\n",
    "        tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "        unigrams = set(tokens)\n",
    "        bigrams = set([tokens[i]+' '+tokens[i+1] for i in range(len(tokens) - 1)])\n",
    "        searchspace = unigrams | bigrams\n",
    "        return len(DL_TERMS&searchspace)>0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import tldextract\n",
    "import bs4\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subpage_result(response):\n",
    "    ans = []\n",
    "    try:\n",
    "        ans.append(has_dl_string_list(extract_words(response['soup_main'])))\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        if response.get('soup_subs') is not None:\n",
    "            try:\n",
    "                for element in response['soup_subs']:\n",
    "                    ans.append(has_dl_string_list(extract_words(element)))\n",
    "            except TypeError:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "    return sum(ans)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "dmap = get_url_map()\n",
    "durls, ddat, dnces = [], [], []\n",
    "dfs = []\n",
    "for url, dat in tqdm(zip(processed_urls, responses)):\n",
    "    durls.append(url)\n",
    "    ddat.append(dat),\n",
    "    dnces.append(dmap[url])\n",
    "ddd = pd.DataFrame({\n",
    "    'url': durls,\n",
    "    'scrape_data': [d.get('soup_main') if isinstance(d, dict) else d for d in ddat],\n",
    "    'nces_id': dnces\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ede42a-bc6a-44d8-82f7-ce62f2506373",
   "metadata": {},
   "source": [
    "## Postprocessing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffec963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import tldextract\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bd6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip parameters and standardize link format\n",
    "def standardize_links(text):\n",
    "    if isinstance(text, bs4.BeautifulSoup):\n",
    "        soup = text\n",
    "    else:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    standardized_links = []\n",
    "\n",
    "    for link in links:\n",
    "        url = link.get('href')\n",
    "        if url:\n",
    "            parsed_url = tldextract.extract(url)\n",
    "            standardized_link = f\"{parsed_url.domain}.{parsed_url.suffix}\"\n",
    "            standardized_links.append(standardized_link)\n",
    "\n",
    "    return standardized_links\n",
    "\n",
    "# Function to extract words from HTML text elements\n",
    "def extract_words(text, remove_sw=True):\n",
    "    if isinstance(text, bs4.BeautifulSoup):\n",
    "        soup = text\n",
    "    else:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "    text_elements = soup.find_all(text=True)\n",
    "    words = []\n",
    "\n",
    "    for element in text_elements:\n",
    "        if element.parent.name not in ['style', 'script', 'head', 'title']:\n",
    "            content = element.strip()\n",
    "            # Remove non-words (such as links)\n",
    "            content = re.sub(r'https?://\\S+', '', content)\n",
    "            # Split into words and convert to lowercase\n",
    "            words.extend(content.lower().split())\n",
    "\n",
    "    if remove_sw:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    return words\n",
    "\n",
    "# Function to detect social media platforms\n",
    "def detect_social_media(text):\n",
    "    if isinstance(text, bs4.BeautifulSoup):\n",
    "        soup = text\n",
    "    else:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "    social_media_results = []\n",
    "\n",
    "    # Add social media platform names and patterns here\n",
    "    social_media_patterns = [\n",
    "        (r\"(?:http|https):\\/\\/(?:www\\.)?twitter\\.com\\/([a-zA-Z0-9_]+)\", \"Twitter\"),\n",
    "        (r\"(?:http|https):\\/\\/(?:www\\.)?facebook\\.com\\/([a-zA-Z0-9.]+)\", \"Facebook\"),\n",
    "        (r\"(?:http|https):\\/\\/(?:www\\.)?instagram\\.com\\/([a-zA-Z0-9_]+)\", \"Instagram\"),\n",
    "        (r\"(?:http|https):\\/\\/(?:www\\.)?linkedin\\.com\\/([a-zA-Z0-9\\-]+)\", \"LinkedIn\")\n",
    "    ]\n",
    "\n",
    "    for pattern, platform in social_media_patterns:\n",
    "        matches = re.findall(pattern, str(soup))\n",
    "        for match in matches:\n",
    "            social_media_results.append((platform, match))\n",
    "\n",
    "    return social_media_results\n",
    "\n",
    "# Example usage\n",
    "def get_langs(s):\n",
    "    s = \" \".join(extract_words(s, remove_sw=False))\n",
    "    ans = cld2.detect(\n",
    "        s, returnVectors=True\n",
    "    )\n",
    "    return ans\n",
    "\n",
    "def process_html_string(html_string):\n",
    "    \n",
    "    if isinstance(html_string, list):\n",
    "        html_string = html_string[0]\n",
    "\n",
    "    # Apply the functions to the input HTML string\n",
    "    try:\n",
    "        standardized_links = standardize_links(html_string)\n",
    "    except:\n",
    "        standardized_links = []\n",
    "    try:\n",
    "        words = extract_words(html_string)\n",
    "    except:\n",
    "        words = []\n",
    "    try:\n",
    "        social_media = detect_social_media(html_string)\n",
    "    except:\n",
    "        social_media = []\n",
    "    try:\n",
    "        langs = get_langs(html_string)\n",
    "    except:\n",
    "        langs = ''\n",
    "\n",
    "    # Create and return the dictionary of results\n",
    "    results = {\n",
    "        \"standardized_links\": standardized_links,\n",
    "        \"words_without_stopwords\": words,\n",
    "        \"social_media_platforms\": social_media,\n",
    "        \"langs\": langs\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "tmp = ddd['scrape_data'].progress_map(process_html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique keys from all dictionaries\n",
    "all_keys = set().union(*(d.keys() for d in tmp))\n",
    "\n",
    "# Create a dictionary with empty lists for each key\n",
    "data_dict = {key: [] for key in all_keys}\n",
    "\n",
    "# Fill the dictionary with data from the list of dictionaries\n",
    "for data in tmp:\n",
    "    for key in all_keys:\n",
    "        data_dict[key].append(data.get(key))\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.concat([ddd, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('1-scraped-data-final-schools.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
